{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"toc_visible":true},"accelerator":"GPU","gpuClass":"standard"},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Assignment 2"],"metadata":{"id":"ZguKQsaEtw45"}},{"cell_type":"markdown","source":["#### Student ID: *Double click here to fill the Student ID*\n","\n","#### Name: *Double click here to fill the name*"],"metadata":{"id":"RxvUeukG506E"}},{"cell_type":"markdown","source":["Firstly, install the following dependencies:"],"metadata":{"id":"xX8YwqcZCk8Z"}},{"cell_type":"code","source":["!pip install shap -qq\n","!pip install lime -qq\n","!pip install imodels -qq\n","!pip install pyngrok -qq\n","!pip install cleanlab -q\n","!pip install dtreeviz -qq"],"metadata":{"id":"3wXbOK6XnqFf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If you are using Colab or Kaggle notebook, try to install tensorflow-model-server and set up the tunnel using the following commands:"],"metadata":{"id":"9KH62SYtC7cU"}},{"cell_type":"code","source":["import sys\n","from pyngrok import ngrok, conf\n","import getpass\n","\n","if \"google.colab\" in sys.modules or \"kaggle_secrets\" in sys.modules:\n","    url = \"https://storage.googleapis.com/tensorflow-serving-apt\"\n","    src = \"stable tensorflow-model-server tensorflow-model-server-universal\"\n","    !echo 'deb {url} {src}' > /etc/apt/sources.list.d/tensorflow-serving.list\n","    !curl '{url}/tensorflow-serving.release.pub.gpg' | apt-key add -\n","    !apt update -q && apt-get install -y tensorflow-model-server\n","    %pip install -q -U tensorflow-serving-api\n","\n","    print(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\")\n","    conf.get_default().auth_token = getpass.getpass()\n","\n","    # Setup a tunnel to the streamlit port 8050\n","    public_url = ngrok.connect(8050)\n","\n","    print(public_url)"],"metadata":{"id":"rVyaENKLAU0K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For this assignment, besides the labs, you may find the assignments in the previous offering useful https://phonchi.github.io/nsysu-math604/."],"metadata":{"id":"a15bc45ZaboV"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.ensemble import RandomForestClassifier\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_breast_cancer"],"metadata":{"id":"jnYu43u87Kpo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q1: Data cleaning with colic dataset"],"metadata":{"id":"XHmRCbF1VtRO"}},{"cell_type":"markdown","source":["<center><img src=\"https://cdn.leonardo.ai/users/f26a2ba8-8273-45e9-8db9-958f83058486/generations/97af1bc3-e1b5-4c6a-84a7-3a8e7a37e97f/Default_Horse_medical_records_and_doctor_0.jpg\"></center>"],"metadata":{"id":"k_pxeBtVtw4z"}},{"cell_type":"markdown","source":["In this problem, we will deal with a colic dataset modified from https://www.kaggle.com/datasets/uciml/horse-colic from our customer. It presents the medical attributes of horses afflicted by colic, detailing their survival outcomes and comprises 300 instances and 26 input variables, along with a single output variable. We will predict whether the problem was surgical or not, making it a binary classification problem. Notably, a significant number of missing values appear across various columns, denoted by a question mark character (\"?\"). "],"metadata":{"id":"Tzp0m9WpYTUb"}},{"cell_type":"markdown","source":["Firstly, execute the following code snippet for data preparation:"],"metadata":{"id":"qVED2vGPI7Hk"}},{"cell_type":"code","source":["# Function for comparing different approaches\n","def scoring_accuracy(X_train, X_valid, y_train, y_valid):\n","    model = RandomForestClassifier(random_state=2023)\n","    model.fit(X_train, y_train)\n","    preds = model.predict(X_valid)\n","    return accuracy_score(y_valid, preds)"],"metadata":{"id":"4OZBmK3bIf8m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv('colic.csv', na_values='?')\n","data.head()"],"metadata":{"id":"w3xUb9QhzT2n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To keep things simple, we'll split the columns into numerical can categorical features\n","mapping_dict = {\n","    'no': 0,\n","    'yes': 1\n","}\n","\n","data[\"surgical_lesion\"] = data[\"surgical_lesion\"].replace(mapping_dict)\n","y = data[\"surgical_lesion\"]\n","data = data.drop(\"surgical_lesion\", axis=1)\n","num_col = data.select_dtypes(exclude=['object'])\n","cat_col = data.select_dtypes(exclude=['int64','float64'])\n","\n","# Divide data into training and validation subsets\n","X_train, X_valid, y_train, y_valid = train_test_split(data, y, train_size=0.8, test_size=0.2, random_state=0)\n","X_train_num = X_train.select_dtypes(exclude=['object'])\n","X_valid_num = X_valid.select_dtypes(exclude=['object'])\n","X_train_cat = X_train.select_dtypes(exclude=['int64','float64'])\n","X_valid_cat = X_valid.select_dtypes(exclude=['int64','float64'])"],"metadata":{"id":"CzSh5f78862p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To ensure reproducibility, please set all the random seeds to 2023:"],"metadata":{"id":"BUNqRpQ177vj"}},{"cell_type":"markdown","source":["#### (a) To simplify the problem, we will consider only the numerical columns first. We will compare the model accuracy between the three data-cleaning approaches. (10%)\n","\n","1. Replace missing values with the mean value along each column using `SimpleImputer()`.\n","2. Use the KNN imputation with 3 neighbors using `KNNImputer()`.\n","3. Use the iterative imputation method and set the regressor as Radnomforest using `IterativeImputer()`.\n","\n","Use the `scoring_accuracy()` function provided above to perform classification and calculate the accuracy for each approach. Finally, make some comments on the results.\n","\n","Hint: Since we are working with both training and validation sets, you should apply the same transform when you impute the missing value for both sets."],"metadata":{"id":"L3ZRsJXwtw5U"}},{"cell_type":"code","source":["# Simple Imputation\n","# coding your answer here."],"metadata":{"id":"Yak3cMBj_i-u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# KNN Imputation\n","# coding your answer here."],"metadata":{"id":"nO2MOKOb_kOO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Iterative Imputation\n","# coding your answer here."],"metadata":{"id":"O7X4fla-C0_V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> Ans: *double click here to answer the question.*"],"metadata":{"id":"3_CayrArMzJr"}},{"cell_type":"markdown","source":["#### (b) Now, we will add categorical variables into consideration. Try to:\n","\n","* Replace missing values with the most frequent value along each column for categorical variables. \n","* Perform label (ordinal) encoding for the categorical variables. \n","* When there are unknown categories in the validation set, set it to -1 for the label encoding.\n","* Use the best approach you found in (a) to impute the numerical column.\n","\n","Combining the transformation above, which contains a separate numerical and categorical pipeline to the original training and validation data split (`X_train` and `X_valid`) and using the `scoring_accuracy()` function to calculate the accuracy. Make some comments on the results when comparing with (a). (10%)\n","\n","Hint: Since we are working with both training and validation sets, try to apply the same transform when you impute the missing data or encode the variables. You may find [ColumnTransformer](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html), [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) and the [`handle_unknown`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) option in the encoder useful."],"metadata":{"id":"HugAnsOHPKVt"}},{"cell_type":"code","source":["# coding your answer here."],"metadata":{"id":"LjSTBaXEKz4l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> Ans: *double click here to answer the question.*"],"metadata":{"id":"7jFGcetvMxba"}},{"cell_type":"markdown","source":["#### (c) Use `CleanLearning` with `RandomForestClassifier` to fit the cleaned training you obtained in (b) (i.e., after imputation and encoding). Then, calculate the accuracy of the validation set. Finally, report the possible label issue in the training set using `cleanlab.dataset.health_summary()`. (10%)"],"metadata":{"id":"k9U3qjndOyRD"}},{"cell_type":"code","source":["# coding your answer here."],"metadata":{"id":"aKm0OA_xMrfL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> Ans: *double click here to answer the question.*"],"metadata":{"id":"DiyB1UQwM0Hb"}},{"cell_type":"markdown","source":["## Q2. Feature engineering and selection with Ames housing dataset"],"metadata":{"id":"9sQ0yGAUQH7A"}},{"cell_type":"markdown","source":["In this question, we are going to examine several feature engineering and feature selection methods.\n","\n","The dataset is from our previous project, which is a modified version of the Ames housing dataset. The original data was compiled by Dean De Cock for use in data science education and published in [De Cock, D. (2011)](https://www.tandfonline.com/doi/abs/10.1080/10691898.2011.11889627). The modified version contains 2930 rows with 79 columns describing every aspect of residential homes in Ames, Iowa. The target of this problem is the `SalePrice`, while all the other columns are treated as features."],"metadata":{"id":"fSWv6if8M053"}},{"cell_type":"markdown","source":["Firstly, execute the following code snippet for data preparation:"],"metadata":{"id":"h-jvQEJdNCDn"}},{"cell_type":"code","source":["# Function for comparing different approaches with cross validation\n","def score_dataset(X, y, pipeline):\n","    # See https://stats.stackexchange.com/questions/27750/feature-selection-and-cross-validation\n","    # , https://stackoverflow.com/questions/56308116/should-feature-selection-be-done-before-train-test-split-or-after\n","    # and https://stats.stackexchange.com/questions/2306/feature-selection-for-final-model-when-performing-cross-validation-in-machine\n","\n","    # For simplicity, we perform label encoding for categoricals here, though it may be better to put it in pipline\n","    for colname in X.select_dtypes([\"category\", \"object\"]):\n","        X[colname], _ = X[colname].factorize()\n","    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n","    score = cross_val_score(\n","        pipeline, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n","    )\n","    score = -1 * score.mean()\n","    score = np.sqrt(score)\n","    return score\n","\n","df = pd.read_csv(\"ames.csv\")\n","X = df.copy()\n","y = X.pop('SalePrice')\n","\n","X.head()"],"metadata":{"id":"BSXA_5EOQJyo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To ensure reproducibility, please set all the random seeds to 2023:"],"metadata":{"id":"40E8YxcNHaBT"}},{"cell_type":"markdown","source":["#### (a) The Ames dataset has a lot of features! Fortunately, you can identify the features with the most potential and use the filtering method. To start with, report the mutual information between the target and each feature in descending order using the `mutual_info_regression()` function on the original dataset `X`. Then, build a pipeline that selects the best 10 features and transforms the dataset using `SelectKBest()` function and fits the resulting dataset using Random forest. Calculate the RMSLE on the transformed dataset using the `score_dataset()`. (10%)\n","\n","\n","Hint: Since we are working with both training and validation sets, you may find [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) useful."],"metadata":{"id":"8-JV8pXMUOxu"}},{"cell_type":"code","source":["# coding your answer here."],"metadata":{"id":"LirUORsKRbJi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### (b) Next, we'll rely on PCA to try to untangle the correlational structure of these features. Firstly, apply PCA on the dataset `X` and plot the cumulative explain variance using `pca()`. Then, build a pipeline that selects the number of components so that the cumulative explained variance is just above 90%  and fits the resulting dataset using Random forest. Calculate the RMSLE on the transformed dataset using the `score_dataset()`. (10%)\n","\n","Hint: PCA is scale sensitive. Therefore, you must standardize the dataset before performing PCA by putting it into the pipeline. Since we are working with both training and validation sets, you may find [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) useful."],"metadata":{"id":"icci_X2BUSED"}},{"cell_type":"code","source":["# coding your answer here."],"metadata":{"id":"wSVkTNI1Z85h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### (c) Finally, we will explore the embedding method. Try to build a pipeline that selects features using `SelectFromModel()` with the feature importance provided by `RandomForestRegressor` and fits the resulting dataset using Random forest. Then, calculate the RMSLE on the transformed dataset using the `score_dataset()`. Finally, comment on the results obtained by comparing it with (a) and (b). (10%)\n","\n","Hint: You do not need to specify `threshold` or `max_features`; just use the default value to determine the number of features automatically. Since we are working with both training and validation sets, you may find [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) useful."],"metadata":{"id":"W2_RKEDIa-mB"}},{"cell_type":"code","source":["# coding your answer here."],"metadata":{"id":"9nXd8BYxa_1g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> Ans: *double click here to answer the question.*"],"metadata":{"id":"0K64FidHNCmi"}},{"cell_type":"markdown","source":["## Q3: Analyze breast cancer dataset with interpretable methods"],"metadata":{"id":"FZWBn2fiCh0J"}},{"cell_type":"markdown","source":["Assume that there is a center that consults us to diagnose breast cancer and has digitized the images of the breast from around 570 patients. Features were computed from these digitized images that described the characteristics of cell nuclei in the images. For each cell nucleus, 10 features are used to describe its characteristics. For all the nuclei present in an image of a patient, the mean, standard error, and the largest or worst values are computed for each of these 10 features. Each patient, therefore, has 30 features in total. Given these input features, the goal of the system is to predict whether the cell is benign or malignant and to provide a confidence score for the doctor to help with their diagnosis.\n","\n"],"metadata":{"id":"1pvXE44BRzOO"}},{"cell_type":"markdown","source":["Firstly, execute the following code snippet for data preparation:"],"metadata":{"id":"rCpaf3DeSoJM"}},{"cell_type":"code","source":["data = load_breast_cancer(as_frame=True)\n","X = data['data']\n","y = data['target']\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=24)\n","X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=24)"],"metadata":{"id":"cKk5jXiPdqPu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train.head()"],"metadata":{"id":"r-Or8aMKd56D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To ensure reproducibility, please set all the random seeds to 2023:"],"metadata":{"id":"MPWjasGTHbcF"}},{"cell_type":"markdown","source":["#### (a) First, fit an interpretable model and show doctors some evidence the model is doing something in line with their medical intuition. (10%)\n","\n","* Try to build a `FIGSClassifier()` decision rule model as a baseline and calculate the accuracy of the model on the validation set. Set the parameter [`max_rule`](https://csinva.io/imodels/tree/figs.html) to 5.\n","* Draw the feature importance plot using the [feature_importances_](https://csinva.io/imodels/tree/figs.html#imodels.tree.figs.FIGS.feature_importances_) and find out the most important feature.\n","* Now, a patient (The data is recorded in the first row in the test dataset with id `331`) comes in, and he/she would like to know why he/she got (or does not get) breast cancer. Use the rule generated from the model to give him/her some reasons. You may also use a tree to visualize the decision path."],"metadata":{"id":"hs7Gkw7hjDyG"}},{"cell_type":"code","source":["# coding your answer here."],"metadata":{"id":"v74j59TB098z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> Ans: *double click here to answer the question.*"],"metadata":{"id":"BfrDQYad_ulf"}},{"cell_type":"markdown","source":["#### (b) The doctor is glad you convinced the patients and would like to know more. It appears `worst area` of the cell is a critical feature, and the doctors would like to know more about that. (10%)\n","\n","* Divide the original data `X` into two datasets, Benign (Target equals 1) and Malignant (Target equals 0), respectively. Draw the distribution of the `worst area` for these two datasets.\n","* Create a partial dependence plot for them that shows how `worst area` of the cell affects the model's predictions. (You should use the validation set to generate the plot)\n","* Comment on your results based on the previous two items about how `worst area` of the cell affect the predictions. Is it consistent with the results from (a)?"],"metadata":{"id":"CLSDeOve2psq"}},{"cell_type":"code","source":["# coding your answer here."],"metadata":{"id":"x_RTjnyzNLCa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> Ans: *double click here to answer the question.*"],"metadata":{"id":"pEPv9RmQAd6w"}},{"cell_type":"markdown","source":["#### (c) The doctor is glad about the results, but he/she is still worried about the model performance of the model you just built.  (5%)\n","\n","* Try to build a more complicated classifier based on a deep neural network (DNN) and train it using the following code. \n","* Then report the accuracy on the validation set and test set.\n","* Find out the patients that the model is most confident and least confident based on the probability from `model.predict()`, and report their index."],"metadata":{"id":"Jg6gxw_yn71n"}},{"cell_type":"code","source":["tf.keras.utils.set_random_seed(2023)\n","# Create a sequential model\n","model = models.Sequential()\n","\n","# Add layers to the model\n","model.add(layers.Dense(20, input_dim=30, activation='relu'))\n","model.add(layers.Dense(10, activation='relu'))\n","model.add(layers.Dense(5, activation='relu'))\n","model.add(layers.Dense(1, activation='sigmoid'))\n","\n","# Display the model summary\n","model.summary()\n","\n","# Compile the model with binary cross-entropy loss and Adam optimizer\n","model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n","\n","# Train the model for 300 epochs and track the history\n","num_epochs = 300\n","history = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_val, y_val), verbose=1)"],"metadata":{"id":"uTRdAxZaV6uA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# coding your answer here."],"metadata":{"id":"i2sB9zSyijfD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> Ans: *double click here to answer the question.*"],"metadata":{"id":"L7DCZvufNVG6"}},{"cell_type":"markdown","source":["#### (d) Use Lime and Shap to explain the two patients you find in (c)\n","* Create the [`lime_tabular.LimeTabularExplainer()`](https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_tabular) and use [`explain_instance()`](https://lime-ml.readthedocs.io/en/latest/lime.html#lime.lime_tabular.LimeTabularExplainer.explain_instance) to explain the two instance. Then, draw the local explanation for these two instances by showing the most important five features.\n","* Create the [`KernelExplainer()`](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html#) using `shap` with the following argument, where `prob` is also defined below:  (15%)\n","\n","```python\n","KernelExplainer(prob, X_train.to_numpy(), link=\"logit\")\n","\n","def prob(data):\n","    return model.predict(data).reshape(-1, 1)\n","```\n","* Using the above [`KernelExplainer()`](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html#) to explain the two instances by calculating the shap value and draw the force plots for them using [`shap.force_plot()`](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.force_plot.html). You can set the parameter [`nsamples`](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html#shap.KernelExplainer.shap_values) to 100 to speed up the calculation.\n","* Based on the results of `lime` and `shap` explain why these two patients got (or does not get) breast cancer. Do the results consistent with (a) and (b)?"],"metadata":{"id":"bMt0xCeDW7vf"}},{"cell_type":"code","source":["# coding your answer here."],"metadata":{"id":"7k4mOGdrFFq5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> Ans: *double click here to answer the question.*"],"metadata":{"id":"vL7Kupn5k63r"}},{"cell_type":"markdown","source":["#### (e) Now, the doctors are convinced you have the right data, and the model overview looks reasonable. It's time to turn this into a finished product they can use. (10%)\n","* Try to use `tf.keras.models.save_model()` to save your DNN model.\n","* Deploy your model as a REST API server using TensorFlow Serving with `tensorflow_model_server`. \n","* Test your server by sending a request that contains the data of the previous two patients using `json.dumps()` and `requests.post()`. Show that the responses from the server are close to the predictions of the original model for these two patients using [`np.isclose()`](https://numpy.org/doc/stable/reference/generated/numpy.isclose.html).\n","\n","Hint: Notice that the input to the DNN model should be with shape `(number of samples, number of features)` even if you feed a single instance."],"metadata":{"id":"otDoP1JUL-Ln"}},{"cell_type":"code","source":["# coding your answer here."],"metadata":{"id":"J_YcHca4HBHs"},"execution_count":null,"outputs":[]}]}