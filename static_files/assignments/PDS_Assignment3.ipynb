{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZguKQsaEtw45"
   },
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxvUeukG506E"
   },
   "source": [
    "#### Student ID: *Double click here to fill the Student ID*\n",
    "\n",
    "#### Name: *Double click here to fill the name*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZWBn2fiCh0J"
   },
   "source": [
    "## Q1: Exploring the TensorFlow playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YMlcXMAnDvQ"
   },
   "source": [
    "[TensorFlow Neural Net Playground](http://playground.tensorflow.org/) is an interactive, web-based visualization tool designed to facilitate a deeper understanding of neural networks and their underlying concepts. It allows users to experiment with various neural network architectures, hyperparameters, and activation functions in real-time, without requiring extensive coding or expertise in machine learning. The Playground features a simple, user-friendly interface that visually represents the neural network's structure and learning process. Users can adjust the number of layers, neurons, learning rate, regularization techniques, and more while observing how these changes impact the network's performance on synthetic datasets.\n",
    "\n",
    "In this exercise, we will explore the web interface and replicate the experiment using `Python`. You are free to use `TensorFlow`, `PyTorch`, or other libraries to complete the exercise.\n",
    "\n",
    "To ensure reproducibility, please set all the random seeds to 2024:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRB--okZqhSK"
   },
   "source": [
    "#### (a) Execute the following steps first: (10%)\n",
    "\n",
    "1. **Reduce the hidden layers to only one layer** and change the activation function to **ReLU**.\n",
    "2. **For each of the four datasets** (Circle, XOR, Two Gaussian, and Spiral), run the model three times. Before each trial, click the **\"Reset the network\"** button to obtain a new random initialization. (The \"Reset the network\" button is the circular reset arrow located just to the left of the Play button.)\n",
    "3. **Allow each trial to run for at least 500 epochs** to ensure convergence.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "- Make some comments about the role of initialization in this non-convex optimization problem.\n",
    "- For each dataset, is it possible to use one hidden layer to achieve convergence results? If yes, what is the minimum number of neurons required (keeping all other parameters unchanged) to ensure that it almost always converges to the global minima (where the test loss is below 0.015)?\n",
    "- Finally, paste the convergence (or inability to converge) results below (each dataset should include one image).\n",
    "\n",
    "Note: The convergence images should display all the settings and the model. An example is available below (the settings are the default, and you need to adjust them according to the problem).\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?id=12MRzcrm1S2yitiyt3mM4il4GVZ8nNFHS\" alt=\"drawing\" width=\"600\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87ZkVNfla2MF"
   },
   "source": [
    "> Ans: *double click here to answer the question.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRz93IUsq8mS"
   },
   "source": [
    "#### (b) Choose a dataset from part (a), execute the following code to import the dataset you selected, and plot the data along with the decision boundary: (10%)\n",
    "\n",
    "```python\n",
    "!pip install git+https://github.com/phonchi/playground-data.git -qq\n",
    "\n",
    "import plygdata as pg\n",
    "from plygdata.playground import Player\n",
    "\n",
    "data_noise = 0\n",
    "validation_data_ratio = 0.5\n",
    "\n",
    "# Choose the dataset you want\n",
    "# data_array = pg.generate_data(pg.DatasetType.ClassifyCircleData, data_noise)\n",
    "data_array = pg.generate_data(pg.DatasetType.ClassifyXORData, data_noise)\n",
    "# data_array = pg.generate_data(pg.DatasetType.ClassifyTwoGaussData, data_noise)\n",
    "# data_array = pg.generate_data(pg.DatasetType.ClassifySpiralData, data_noise)\n",
    "\n",
    "X_train, y_train, X_valid, y_valid = pg.split_data(data_array, validation_size=validation_data_ratio)\n",
    "\n",
    "# Plot the data using the Playground style\n",
    "fig, ax = pg.plot_points_with_playground_style(\n",
    "    X_train, y_train, X_valid, y_valid, figsize=(6, 6), dpi=100\n",
    ")\n",
    "\n",
    "# Draw the decision boundary for the X1 input feature\n",
    "pg.draw_decision_boundary(fig, ax, node_id=pg.InputType.X1, discretize=False)\n",
    "```\n",
    "\n",
    "Now, build the Deep Neural Network (DNN) you designed in part (a) for **one of the selected dataset**. Train the DNN using the Stochastic Gradient Descent (SGD) optimizer and the Mean Squared Error (MSE) loss function. Report the final accuracy on the validation set and plot the decision boundary using the following code:\n",
    "\n",
    "```python\n",
    "fig, ax = pg.plot_points_with_playground_style(\n",
    "    X_train, y_train, X_valid, y_valid, figsize=(6, 6), dpi=100\n",
    ")\n",
    "xx = Player.get_boundary_array()\n",
    "pred = model.predict(xx)  # Alternatively, use model(xx)\n",
    "pg.draw_decision_boundary(fig, ax, node_id=pg.InputType.X1, prob=pred, discretize=False)\n",
    "```\n",
    "\n",
    "Additionally, plot the learning curve (loss vs. epochs) during training. Do your results align with those from part (a)?\n",
    "\n",
    "**Hint:** The labels are `-1` and `1` by default in the Playground. Therefore, you should use the `tanh` activation function in the final layer. Although the sigmoid activation function with a cross-entropy loss is typically used for classification problems, using `tanh` with MSE is requested for this exercise. You can calculate the accuracy using the following code:\n",
    "\n",
    "```python\n",
    "from keras import backend as K\n",
    "\n",
    "def custom_accuracy(y_true, y_pred):\n",
    "    # Map tanh outputs to {-1, 1}\n",
    "    y_pred_class = K.sign(y_pred)\n",
    "    # Handle zero values (optional)\n",
    "    y_pred_class = K.switch(K.equal(y_pred_class, 0), K.ones_like(y_pred_class), y_pred_class)\n",
    "    # Ensure y_true is float32 for comparison\n",
    "    y_true = K.cast(y_true, dtype='float32')\n",
    "    # Calculate accuracy\n",
    "    return K.mean(K.equal(y_true, y_pred_class))\n",
    "\n",
    "def custom_mse_loss(y_true, y_pred):\n",
    "    # Calculate the squared differences\n",
    "    squared_diff = K.square(y_true - y_pred)\n",
    "    # Compute the mean of squared differences\n",
    "    mse = K.mean(squared_diff, axis=-1)\n",
    "    # Multiply by 1/2\n",
    "    return 0.5 * mse\n",
    "\n",
    "# Replace 'your_optimizer' with the optimizer used in part (a)\n",
    "model.compile(optimizer=your_optimizer, loss=custom_mse_loss, metrics=[custom_accuracy])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YuHfnRyrrA-5"
   },
   "outputs": [],
   "source": [
    "# coding your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNmXS61c09sp"
   },
   "source": [
    "> Ans: *double click here to answer the question.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWuQysqEcPix"
   },
   "source": [
    "#### (c) Execute the following steps first: (10%)\n",
    "\n",
    "1. Change the dataset to the **Spiral** (bottom-right dataset under the \"DATA\" panel) and adjust the training to test data ratio to **60% training** and **40% testing**. Set the **batch size** to **12**.\n",
    "2. Increase the **noise level** to **50** while keeping the training and test set ratio unchanged.\n",
    "3. Train the best model you can. Feel free to add or remove layers and neurons. You can also adjust learning settings such as learning rate, regularization rate, activation functions, and batch size. Additionally, you may increase the input features to include interaction terms or other relevant features. Aim to achieve a **test loss below 0.15**.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "- How many parameters does your model have?\n",
    "- Describe the model architecture and the training strategy you used.\n",
    "- Paste the convergence results below.\n",
    "\n",
    "Note: You may need to train the model for a sufficient number of epochs and manually implement learning rate scheduling to achieve the desired test loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrSQE4nsa2MI"
   },
   "source": [
    "> Ans: *double click here to answer the question.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0iqOgRMwjhN"
   },
   "source": [
    "#### (d) Execute the following code to import the spiral dataset and plot the data and decision boundary: (10%)\n",
    "\n",
    "```python\n",
    "data_noise = 0.5\n",
    "validation_data_ratio = 0.4\n",
    "\n",
    "# Generate data\n",
    "data_array = pg.generate_data(pg.DatasetType.ClassifySpiralData, data_noise)\n",
    "\n",
    "X_train, y_train, X_valid, y_valid = pg.split_data(data_array, validation_size=validation_data_ratio)\n",
    "\n",
    "# Plot the data on the standard graph for Playground\n",
    "fig, ax = pg.plot_points_with_playground_style(X_train, y_train, X_valid, y_valid, figsize=(6, 6), dpi=100)\n",
    "pg.draw_decision_boundary(fig, ax, node_id=pg.InputType.X1, discretize=False)\n",
    "```\n",
    "\n",
    "Now, build the Deep Neural Network (DNN) you designed in part (c) and train the DNN using the SGD optimizer. Report the final accuracy on the validation set and plot the decision boundary using the following code:\n",
    "\n",
    "```python\n",
    "fig, ax = pg.plot_points_with_playground_style(X_train, y_train, X_valid, y_valid, figsize=(6, 6), dpi=100)\n",
    "xx = Player.get_boundary_array()\n",
    "pred = model.predict(xx)  # or model(xx)\n",
    "pg.draw_decision_boundary(fig, ax, node_id=pg.InputType.X1, prob=pred, discretize=False)\n",
    "```\n",
    "\n",
    "Plot the learning curve (loss vs. epochs) during training. Do your results match those from part (c)?\n",
    "\n",
    "**Hint:** If you would like to perform feature engineering similar to the Playground, consider using [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) or [`FunctionTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZ0RjQQaPqXm"
   },
   "outputs": [],
   "source": [
    "# coding your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-F8Vxf11CpJ"
   },
   "source": [
    "> Ans: *double click here to answer the question.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwIerksk65Ru"
   },
   "source": [
    "#### (e) You may find that the learning curve obtained in part (d) is noisy and requires many epochs to converge. Try to improve the DNN from part (d) by modifying the network architecture, adjusting the learning rate schedule, or changing the optimizer to achieve a smoother learning curve and faster convergence using `Python` programming. (10%)\n",
    "\n",
    "Finally, plot the learning curve during training and the decision boundary using the following code:\n",
    "\n",
    "```python\n",
    "fig, ax = pg.plot_points_with_playground_style(X_train, y_train, X_valid, y_valid, figsize=(6, 6), dpi=100)\n",
    "xx = Player.get_boundary_array()\n",
    "pred = model.predict(xx)  # or model(xx)\n",
    "pg.draw_decision_boundary(fig, ax, node_id=pg.InputType.X1, prob=pred, discretize=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpfVLE0P1ptp"
   },
   "outputs": [],
   "source": [
    "# coding your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BHuiK8mtw5U"
   },
   "source": [
    "## Q2: Exploring the CNN Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obleIPRdcPi1"
   },
   "source": [
    "[CNN Explainer](https://poloclub.github.io/cnn-explainer/) is an interactive, open-source visualization tool designed to provide a comprehensive understanding of Convolutional Neural Networks (CNNs). The explainer aims to demystify the inner workings of CNNs through visualizations and step-by-step explanations. The platform offers a guided walkthrough of the building blocks of CNNs, including convolutional layers, activation functions, pooling layers, and fully connected layers. It allows users to interactively explore the components, visualize feature maps, and understand the effects of different hyperparameters on the network's performance.\n",
    "\n",
    "In this exercise, we will explore the web interface and replicate the experiment using `Python`. You are free to use `TensorFlow`, `PyTorch`, or other libraries to complete the exercise.\n",
    "\n",
    "To ensure reproducibility, please set all the random seeds to 2024:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ngx3M8SStw5U"
   },
   "source": [
    "#### (a) Explore the CNN Explainer and answer the following questions: (10%)\n",
    "\n",
    "1. What is the shape of the input and output of the network?\n",
    "2. What are the kernel size, stride, padding, and number of filters used in the convolutional layer?\n",
    "3. What are the kernel size, stride, and number of filters used in the pooling layer?\n",
    "4. What are the activation functions used in the hidden layer and the output layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1FHzS4hfJTw"
   },
   "source": [
    "> Ans: *double click here to answer the question.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3ZRsJXwtw5U"
   },
   "source": [
    "#### (b) Based on your observations in part (a), build the same CNN using `Python`. Report the total number of parameters and the architecture using the `summary()` function. Remember to rescale or normalize the input before feeding it into the network. (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vs4aGKJ8G_Hr"
   },
   "outputs": [],
   "source": [
    "# coding your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQjg2IUjjbT9"
   },
   "source": [
    "#### (c) Download the dataset from our course website and load the training, validation, and testing datasets from the folders `train_images`, `val_images`, and `test_images`, respectively. Remember to resize the images to $64 \\times 64$ and set the batch size to 32. Finally, select and plot 9 random samples from the training set. (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUx-PTLN9_m6"
   },
   "outputs": [],
   "source": [
    "# coding your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgMwKcwUJorQ"
   },
   "source": [
    "#### (d) Fit the model from part (b) using the following steps: (10%)\n",
    "\n",
    "1. **Add callbacks** to monitor the validation loss, save the best model based on the validation loss, and reduce the learning rate by a factor of 0.5 if the validation loss does not improve for 15 epochs.\n",
    "2. **Train the model** you built in part (b) using the Adam optimizer for 100 epochs with a learning rate of 0.001.\n",
    "3. **Plot the learning curve** after training.\n",
    "4. **Reload the best model** and report the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_12eDOV2tw5Z"
   },
   "outputs": [],
   "source": [
    "# coding your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3MEtFS_49WS"
   },
   "source": [
    "> Ans: *double click here to answer the question.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1TICQ99_TbP"
   },
   "source": [
    "#### (e) The learning curves from part (d) indicate that the model is overfitting. To mitigate this, enhance your model by incorporating data augmentation techniques. Specifically, apply the following augmentations: (10%)\n",
    "\n",
    "- **Horizontal Flipping:** Apply with a 50% probability.\n",
    "- **Shift, Scale, and Rotate:** Shift the image by 5%, scale by 10%, and rotate between -30° to 30° with a 50% probability. (Refer to [ShiftScaleRotate](https://albumentations.ai/docs/api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.ShiftScaleRotate))\n",
    "- **Adjust Contrast and Brightness:** Randomly modify contrast and brightness so that image values range between 0.8 and 1.2 times the original.\n",
    "(Refer to [RandomBrightnessContrast](RandomBrightnessContrast))\n",
    "\n",
    "**Questions**:\n",
    "\n",
    "1. **Integrate Data Augmentation:** Add the specified augmentation techniques to your data preprocessing pipeline.\n",
    "2. **Train the Augmented Model:** Use the same training configuration as in part (d).\n",
    "3. **Compare Performance:** Analyze and compare the learning curves and performance metrics of the augmented model against the previous model.\n",
    "4. **Report Test Accuracy:** Reload the best-performing augmented model and report its accuracy on the test set.\n",
    "\n",
    "Note: Ensure that data augmentation is only applied to the training data and not to the validation or test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BF67H5QBSc8a"
   },
   "outputs": [],
   "source": [
    "# coding your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEM8tdWua2MS"
   },
   "source": [
    "> Ans: *double click here to answer the question.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejdrb2IYJi1B"
   },
   "source": [
    "#### (f) Using the CNN model you built in part (b), predict the class labels of the example images located in the `examples` folder extracted in part (c). Before making predictions, ensure that each image is properly resized to match the input dimensions of your network and normalized according to the preprocessing steps used during training. After obtaining the predictions, calculate the accuracy of your model on these example images and compare your results with the accuracy reported by the CNN Explainer. (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaHVeIGQj4Sb"
   },
   "outputs": [],
   "source": [
    "# coding your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tglMDf_u1H_h"
   },
   "source": [
    "> Ans: *double click here to answer the question.*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
