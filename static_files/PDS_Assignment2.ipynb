{"cells":[{"cell_type":"markdown","metadata":{"id":"ZguKQsaEtw45"},"source":["# Assignment 2"]},{"cell_type":"markdown","metadata":{"id":"RxvUeukG506E"},"source":["#### Student ID: *Double click here to fill the Student ID*\n","\n","#### Name: *Double click here to fill the name*"]},{"cell_type":"markdown","metadata":{"id":"XHmRCbF1VtRO"},"source":["## Q1: Data cleaning with the Melbourne Housing Dataset"]},{"cell_type":"markdown","metadata":{"id":"Tzp0m9WpYTUb"},"source":["In this question, you will practice data preparation skills that are often used in real-world projects.\n","\n","The dataset is a snapshot of the [Melbourne Housing Market dataset](https://www.kaggle.com/datasets/anthonypino/melbourne-housing-market) created by Tony Pino. It was scraped from publicly available results posted weekly on [Domain.com.au](https://www.domain.com.au/). The dataset includes the address, type of real estate, suburb, method of selling, number of rooms, price, real estate agent, date of sale, and distance from the central business district, among other variables."]},{"cell_type":"markdown","metadata":{"id":"qVED2vGPI7Hk"},"source":["Firstly, execute the following code snippet for data preparation (Remember to upload the dataset and import the dependent library first):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w3xUb9QhzT2n"},"outputs":[],"source":["# Function for comparing different approaches\n","def scoring_mape(X_train, X_valid, y_train, y_valid):\n","    model = RandomForestRegressor(n_estimators=10, random_state=2024)\n","    model.fit(X_train, y_train)\n","    preds = model.predict(X_valid)\n","    return mean_absolute_percentage_error(y_valid, preds)\n","\n","# Prepare data\n","df = pd.read_csv('melb_data.csv')\n","y = df.Price\n","\n","# To keep things simple, we'll split the columns into numerical can categorical features\n","data = df.drop(['Price', 'Date', 'Address'], axis=1)\n","num_col = data.select_dtypes(exclude=['object'])\n","cat_col = data.select_dtypes(exclude=['int64','float64'])\n","\n","# Divide data into training and validation subsets, try to use the resulting datafram below in the following questions\n","X_train, X_valid, y_train, y_valid = train_test_split(data, y, train_size=0.8, test_size=0.2, random_state=2024)\n","# Numerical columns\n","X_train_num = X_train.select_dtypes(exclude=['object'])\n","X_valid_num = X_valid.select_dtypes(exclude=['object'])\n","# Categorical columns\n","X_train_cat = X_train.select_dtypes(exclude=['int64','float64'])\n","X_valid_cat = X_valid.select_dtypes(exclude=['int64','float64'])"]},{"cell_type":"markdown","metadata":{"id":"BUNqRpQ177vj"},"source":["To ensure reproducibility, please set all the random seeds to 2024:"]},{"cell_type":"markdown","metadata":{"id":"cdf1Xm9Anl1M"},"source":["### (a) Identify Columns with Missing Values and Calculate Their Missing Rates (5%)\n","Firstly, identify which columns contain missing values. Then, calculate the percentage of missing values in the dataset.\n","\n","Hint: You should calculate the missing rates based on the original `data` matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IT1wAeR8AnAa"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"aQnfzdTFAorp"},"source":["> Ans: *double click here to answer the question.*"]},{"cell_type":"markdown","metadata":{"id":"L3ZRsJXwtw5U"},"source":["### (b) Comparing Model Accuracy Using Three Data-Cleaning Approaches (10%)\n","\n","To simplify the problem, we will consider only the numerical columns. Compare the model accuracy between the following three data-cleaning approaches:\n","\n","1. **Removing all columns with missing values.**\n","2. **Replacing missing values with the median value of each column.**\n","3. **Using the iterative imputation method with a KNN regressor (15 neighbors) via [`IterativeImputer()`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html).**\n","\n","Use the `scoring_mape()` function provided above to perform regression and calculate the Mean Absolute Percentage Error (MAPE) for each approach. Finally, provide comments on the results.\n","\n","**Hint:** Since we are working with both training and validation sets, you should apply the same transformation when imputing missing values for both sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r6UAb6WyAwuJ"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"ZqiACiHKAxCv"},"source":["> Ans: *double click here to answer the question.*"]},{"cell_type":"markdown","metadata":{"id":"HugAnsOHPKVt"},"source":["### (c) Incorporating Categorical Variables (10%)\n","\n","Now, we will consider categorical variables. Try to:\n","\n","1. **Replace missing values with the most frequent value in each column for categorical variables.**\n","2. **Perform target encoding for the categorical variables.**\n","3. **Use the best approach you found in (b) to impute the numerical columns.**\n","\n","Combine the above transformations, which include separate numerical and categorical pipelines, with the original training and validation data splits (`X_train` and `X_valid`). Then, use the `scoring_mape()` function to calculate the Mean Absolute Percentage Error (MAPE). Finally, provide comments on the results compared to those in part (b). (10%)\n","\n","**Hint:** Since we are working with both training and validation sets, apply the same transformations when imputing missing data or encoding variables. You may find [ColumnTransformer](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html), [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), and be sure to enable [cross-fitting](https://scikit-learn.org/stable/modules/preprocessing.html#target-encoder) in target encoding."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Gtsb5tI9Wmq"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"dmGT4g9CA4RC"},"source":["> Ans: *double click here to answer the question.*"]},{"cell_type":"markdown","metadata":{"id":"k9U3qjndOyRD"},"source":["### (d) Identifying Label Issues with CleanLearning and Evaluating Model Accuracy (10%)\n","\n","Use [`CleanLearning`](https://docs.cleanlab.ai/v2.5.0/cleanlab/regression/learn.html#cleanlab.regression.learn.CleanLearning) with a `RandomForestRegressor` to identify possible label issues in the training set. Then, fit `CleanLearning` on the preprocessed training dataset obtained in part (c) (i.e., after imputation and encoding). Finally, calculate the model's accuracy on the validation set. Provide comments on the results compared to those in part (c)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Tlh2WrsNowj"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"ERz9bXyVA8sh"},"source":["> Ans: *double click here to answer the question.*"]},{"cell_type":"markdown","metadata":{"id":"9sQ0yGAUQH7A"},"source":["## Q2. Feature Engineering and Selection with the Ames Housing Dataset"]},{"cell_type":"markdown","metadata":{"id":"fSWv6if8M053"},"source":["In this question, we will examine several feature engineering and feature selection methods.\n","\n","The dataset is a modified version of the Ames Housing Dataset. The original data was compiled by Dean De Cock for use in data science education and published in [De Cock, D. (2011)](https://www.tandfonline.com/doi/abs/10.1080/10691898.2011.11889627). The modified version contains 2,930 rows with 79 columns describing various aspects of residential homes in Ames, Iowa. The target variable for this problem is `SalePrice`, while all other columns are treated as features."]},{"cell_type":"markdown","metadata":{"id":"h-jvQEJdNCDn"},"source":["Firstly, execute the following code snippet for data preparation (Remember to upload the dataset and import the dependent library first):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Ak0r-rgEjlu"},"outputs":[],"source":["def score_rmsle(X, y, model):\n","    \"\"\"\n","    Encodes categorical variables using OrdinalEncoder, splits the data, trains the model,\n","    and returns the Root Mean Squared Log Error (RMSLE) on the validation set.\n","\n","    Parameters:\n","    - X: pd.DataFrame, feature matrix\n","    - y: pd.Series or array-like, target variable\n","    - model: scikit-learn estimator, regression model or pipeline to train\n","\n","    Returns:\n","    - score: float, RMSLE on the validation set\n","    \"\"\"\n","    # See https://stats.stackexchange.com/questions/27750/feature-selection-and-cross-validation\n","    # , https://stackoverflow.com/questions/56308116/should-feature-selection-be-done-before-train-test-split-or-after\n","    # and https://stats.stackexchange.com/questions/2306/feature-selection-for-final-model-when-performing-cross-validation-in-machine\n","\n","    # Split the data into training and validation sets\n","    X_train, X_valid, y_train, y_valid = train_test_split(\n","        X.values, y, train_size=0.8, test_size=0.2, random_state=2024\n","    )\n","\n","    # Train the model\n","    model.fit(X_train, y_train)\n","\n","    # Make predictions on the validation set\n","    preds = model.predict(X_valid)\n","\n","    # Calculate RMSLE\n","    score = root_mean_squared_log_error(y_valid, preds)\n","\n","    return score\n","\n","\n","df = pd.read_csv(\"ames.csv\")\n","X = df.copy()\n","y = X.pop('SalePrice')\n","\n","# For simplicity, we perform label encoding for categoricals here, though it may be better to put it in pipline\n","# Identify categorical columns\n","categorical_cols = X.select_dtypes(include=[\"category\", \"object\"]).columns\n","\n","if len(categorical_cols) > 0:\n","    # Initialize OrdinalEncoder\n","    encoder = OrdinalEncoder(encoded_missing_value=-1)\n","\n","    # Fit and transform the categorical columns\n","    X[categorical_cols] = encoder.fit_transform(X[categorical_cols])"]},{"cell_type":"markdown","metadata":{"id":"40E8YxcNHaBT"},"source":["To ensure reproducibility, please set all the random seeds to 2024:"]},{"cell_type":"markdown","metadata":{"id":"Dhbl8cYfl3Cy"},"source":["### (a) K-Means Clustering as a Feature Engineering method (10%)\n","\n","First, perform k-means clustering using the following parameters:\n","\n","- **Features:** `FirstFlrSF`, `SecondFlrSF`, `GrLivArea`, `LotArea`, `TotalBsmtSF`\n","- **Number of clusters:** 6\n","\n","Next, add the k-means labels and cluster distance features to your original dataset. Finally, perform regression using `RandomForestRegressor` with 10 estimators and calculate the Root Mean Log Squared Error (RMLSE) using the `scoring_rmsle()` function.\n","\n","**Hint:** The `predict()` and `transform()` methods in the [k-means function](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) may be useful when generating the features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3z0I1LhG6hs4"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"8-JV8pXMUOxu"},"source":["### (b) Feature Selection Using Mutual Information (10%)\n","\n","The Ames dataset contains a large number of features. Fortunately, you can identify the most promising features and apply a filtering method for feature selection.\n","\n","1. **Calculate Mutual Information:** Using the dataset obtained in part (a) (with added features), report the mutual information between the target variable (`SalePrice`) and each feature in descending order using the [`mutual_info_regression()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html) function.\n","\n","2. **Build a Feature Selection Pipeline:**\n","   - **Select Best Features:** Use the [`SelectKBest()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) function to select the top 20 features based on mutual information.\n","   - **Fit the Model:** Fit a `RandomForestRegressor` with 10 estimators on the transformed dataset.\n","   \n","3. **Evaluate the Model:** Calculate the Root Mean Squared Logarithmic Error (RMSLE) on the transformed dataset using the `score_rmsle()` function.\n","\n","**Hint:** Since we are working with both training and validation sets, you may find [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) useful."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LirUORsKRbJi"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"C3r5GpY7BUDZ"},"source":["> Ans: *double click here to answer the question.*"]},{"cell_type":"markdown","metadata":{"id":"W2_RKEDIa-mB"},"source":["### (c) Feature Selection Using the Wrapper Method with [Boruta](https://github.com/scikit-learn-contrib/boruta_py) (10%)\n","\n","Now, we will apply the wrapper method for feature selection using [Boruta](https://pdfs.semanticscholar.org/ecc2/ca3150dc4d4d8dceedab244114f191e05742.pdf). Please follow the steps below:\n","\n","1. **Perform Feature Selection with Boruta:**\n","   - **Model Configuration:** Use `RandomForestRegressor()` with `max_depth` set to 5 and `n_estimators` set to 10 as the underlying model in Boruta.\n","   - **Execute Boruta:** Apply Boruta to perform feature selection on the dataset obtained in part (a).\n","\n","2. **Analyze Selected Features:**\n","   - **Number of Features Selected:** Report how many features Boruta selects in this example.\n","   - **Consistency Check:** Determine whether the selected features are consistent with the top 20 features identified in part (b).\n","\n","3. **Model Training and Evaluation:**\n","   - **Regression Model:** Use the selected features to perform regression using `RandomForestRegressor` with 10 estimators.\n","   - **Calculate RMSLE:** Compute the Root Mean Squared Logarithmic Error (RMSLE) using the `scoring_rmsle()` function on the validation set.\n","\n","**Hint:** Since we are working with both training and validation sets, you may find [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) useful."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9nXd8BYxa_1g"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"-Rlxq9_5O0tq"},"source":["> Ans: *double click here to answer the question.*"]},{"cell_type":"markdown","metadata":{"id":"FZWBn2fiCh0J"},"source":["## Q3: Analyzing Hospital Readmission Dataset with Interpretable Methods\n"]},{"cell_type":"markdown","metadata":{"id":"1pvXE44BRzOO"},"source":["Hospital readmission occurs when a patient who has been discharged from a hospital is admitted again within a specified time interval. Generally, a higher readmission rate indicates the ineffectiveness of treatment during previous hospitalizations.\n","\n","Therefore, the hospital seeks your assistance in identifying patients at the highest risk of being readmitted. While doctors will make the final decision about when to discharge each patient, they hope you can build a model to highlight factors that doctors should consider when making discharge decisions. The hospital has provided relevant patient medical information. The given dataset contains the following features:\n","\n","- **Prediction Target:** The prediction target is `readmitted`.\n","- **Number of Inpatient Visits:** Feature names like `number_inpatient` refer to the number of inpatient visits of the patient in the year preceding the encounter.\n","- **Diagnostic Codes:** Features containing the word `diag` indicate the diagnostic code of the illness or illnesses with which the patient was admitted. For example, `diag_1_428` means the patient's first illness diagnosis is code \"428\".\n","- **Medication Indicators:** A feature named `metformin_No` means the patient did not take the medication `metformin`. If this feature has a value of `False`, then the patient did take the drug `metformin`.\n","- **Medical Specialties:** Features that begin with `medical_specialty` describe the specialty of the doctor treating the patient. The values in these fields are all `True` or `False`.\n","\n","**Note:** Our dataset is a cleaned and truncated version of the [Diabetes 130-US Hospitals for Years 1999-2008](https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) dataset from the UCI Machine Learning Repository. Please refer to the above link for a detailed description of the features."]},{"cell_type":"markdown","metadata":{"id":"O8HYYzLT9H7s"},"source":["Firstly, execute the following code snippet for data preparation (Remember to upload the dataset and import the dependent library first):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cKk5jXiPdqPu"},"outputs":[],"source":["X = pd.read_csv('hospital.csv')\n","#  Removes all characters from the column names that are not letters, numbers, or underscores.\n","X = X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n","\n","\n","y = X.readmitted\n","X.drop(['readmitted'], axis=1, inplace=True)\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=2024)"]},{"cell_type":"markdown","metadata":{"id":"MPWjasGTHbcF"},"source":["To ensure reproducibility, please set all the random seeds to 2024:"]},{"cell_type":"markdown","metadata":{"id":"hs7Gkw7hjDyG"},"source":["#### (a) Fit an Interpretable Model to Align with Medical Intuition (10%)\n","\n","1. **Build a FIGSClassifier Model:**\n","   - **Model Construction:** Build a [`FIGSClassifier`](https://csinva.io/imodels/tree/figs.html#imodels.tree.figs.FIGSClassifier) decision rule model as a baseline.\n","   - **Parameters:** Set the parameters `max_rule` to 5 and `max_tree` to 1.\n","   - **Model Evaluation:** Calculate the accuracy of the model on the validation set.\n","\n","2. **Analyze Feature Importance:**\n","   - **Plot Feature Importances:** Draw the feature importance plot using the [`feature_importances_`](https://csinva.io/imodels/tree/figs.html#imodels.tree.figs.FIGS.feature_importances_) attribute.\n","   - **Identify Key Feature:** Determine the most important feature based on the plot.\n","\n","3. **Explain Readmission Decision for a Specific Patient:**\n","   - **Patient Identification:** Consider a patient whose data is recorded in the first row of the validation dataset with ID `20556`.\n","   - **Provide Explanation:** Using the rules generated from the `FIGSClassifier` model, explain to the patient the reasons for their readmission or lack thereof.\n","   - **Visualization:** Optionally, use a tree diagram to visualize the decision path taken by the model for this patient.\n","\n","**Hint:** Ensure that the `FIGSClassifier` model is properly trained and validated before interpreting the results. Visualization tools such as tree diagrams can aid in making the decision process transparent and understandable for medical professionals."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v74j59TB098z"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"BfrDQYad_ulf"},"source":["> Ans: *double click here to answer the question.*"]},{"cell_type":"markdown","metadata":{"id":"CLSDeOve2psq"},"source":["### (b) Enhancing Model Performance with `ExplainableBoostingClassifier` (10%)\n","\n","The doctor is pleased that you have successfully convinced the patients; however, he remains concerned about the performance of the `FIGSClassifier` model you previously built.\n","\n","1. **Build an `ExplainableBoostingClassifier` Model:**\n","   - **Model Construction:** Build a more sophisticated classifier using the [`ExplainableBoostingClassifier`](https://interpret.ml/docs/python/api/ExplainableBoostingClassifier.html).\n","   - **Training Parameters:** Train the model with three automatically selected interaction terms to capture complex relationships between features.\n","\n","2. **Evaluate Model Performance:**\n","   - **Model Evaluation:** Calculate and report the accuracy of the `ExplainableBoostingClassifier` model on the validation set.\n","\n","3. **Provide Model Explanations:**\n","   - **Global Explanation:** Display the feature importance plot to showcase which features are most influential in the model's predictions.\n","   - **Local Explanation:** Generate a local explanation for the patient identified in part (a) to illustrate the specific factors contributing to their readmission prediction.\n","\n","4. **Consistency Analysis:**\n","   - **Compare Explanations:** Assess whether the explanations provided by the `ExplainableBoostingClassifier` are consistent with those from the `FIGSClassifier` model built in part (a).\n","\n","**Hint:** Utilize visualization tools provided by the `interpret` library to effectively illustrate both global and local explanations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTkrPzoh4sEk"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"pEPv9RmQAd6w"},"source":["> Ans: *double click here to answer the question.*"]},{"cell_type":"markdown","metadata":{"id":"bMt0xCeDW7vf"},"source":["\n","### (c) Explaining Patient Readmission Using LIME and SHAP (10%)\n","\n","1. **Explain the Model Using LIME:**\n","   - **Create a LIME Explainer:** Instantiate a [`lime_tabular.LimeTabularExplainer()`](https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_tabular) using the training data.\n","   - **Generate Explanation:** Use the [`explain_instance()`](https://lime-ml.readthedocs.io/en/latest/lime.html#lime.lime_tabular.LimeTabularExplainer.explain_instance) method to explain the `ExplainableBoostingClassifier` model you trained in part (b) for the specific patient mentioned in (a).\n","   - **Visualize Local Explanation:** Draw the local explanation for this patient by displaying the five most important features influencing their readmission prediction.\n","\n","2. **Explain the Model Using SHAP:**\n","   - **Create a SHAP Explainer:** Define the following function and instantiate a [`shap.KernelExplainer()`](https://shap.readthedocs.io/en/latest/generated/shap.KernelExplainer.html) using the function where ebm is the `ExplainableBoostingClassifier` model you trained in part (b):\n","\n","     ```python\n","     def ebm_predict_proba(X):\n","         X_df = pd.DataFrame(X, columns=feature_names)\n","         return ebm.predict_proba(X_df)\n","     \n","     shap_explainer = shap.KernelExplainer(ebm_predict_proba, data=X_train, link=\"logit\", feature_names=list(X_train.columns))\n","     ```\n","\n","   - **Generate SHAP Values:** Use the above `KernelExplainer` to calculate the SHAP values for the `ExplainableBoostingClassifier` model trained in part (b) for the patient.\n","   - **Visualize SHAP Force Plot:** Create force plots for the patient using [`shap.force_plot()`](https://shap.readthedocs.io/en/latest/generated/shap.plots.force.html), setting the parameter `nsamples` to 100 to speed up the calculation.\n","\n","3. **Compare Explanations and Assess Consistency:**\n","   - **Analyze Explanations:** Based on the results from both LIME and SHAP, explain why the patient was (or was not) readmitted.\n","   - **Consistency Check:** Discuss whether the explanations from LIME and SHAP are consistent with the interpretations from parts (a) and (b).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qh0ubAn-ouku"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"vL7Kupn5k63r"},"source":["> Ans: *double click here to answer the question.*"]},{"cell_type":"markdown","metadata":{"id":"otDoP1JUL-Ln"},"source":["### (d) Deploying the Model as a Service with `BentoML` (10%)\n","\n","Now that the doctors are convinced you have the right data and the model overview looks reasonable, it's time to turn this into a finished product they can use.\n","\n","1. **Save and Reload the Model Using BentoML:**\n","   - **Save the Model:** Use [`BentoML`](https://docs.bentoml.com/en/latest/index.html) to save your `ExplainableBoostingClassifier` model.\n","   - **Reload the Model:** Reload the saved model to ensure it has been correctly serialized and deserialized.\n","\n","2. **Verify Model Consistency:**\n","   - **Make Inferences:** Use the reloaded model to make inferences on the patient identified in part (a).\n","   - **Compare Results:** Demonstrate that the inference results from the reloaded model are identical to those from the original model for the same patient.\n","\n","3. **Wrap the Model as a Service:**\n","   - **Write a Service Script:** Develop a script to wrap your model as a [BentoML service](https://docs.bentoml.com/en/latest/guides/services.html).\n","   - **Start the Server:** Deploy the service by starting the BentoML server.\n","   - **Test the Service:**\n","     - **Prepare the Request:** Use the following code snippet to send a request containing the patient's data (You may need to modify the code to be compatible with your server script):\n","       ```python\n","       import json\n","       import requests\n","       import pandas as pd\n","\n","       # Select the first patient from the validation set\n","       df_row = X_valid.iloc[0:1]\n","       input_series_json = df_row.to_json(orient='values')  # Example format: '[[0,1,2,...,63]]'\n","       input_series = json.loads(input_series_json)\n","\n","       # Prepare the input data for the POST request\n","       input_data = {\n","           \"input_series\": input_series\n","       }\n","\n","       # Send the POST request to the BentoML service\n","       response = requests.post(\n","           \"http://127.0.0.1:8050/classify\",\n","           json=input_data\n","       ).text\n","\n","       print(response)\n","       ```\n","     - **Validate the Response:** Ensure that the responses from the server closely match the predictions of the original model for the patient.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XH8NS1L1gnre"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"dGXaazg0BtdI"},"source":["> Ans: *double click here to answer the question.*"]},{"cell_type":"markdown","metadata":{"id":"it8Z1vvKBvRS"},"source":["The following code may be useful for starting and closing the server:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QFH0LDd1hDpG"},"outputs":[],"source":["from pyngrok import ngrok, conf\n","import getpass\n","\n","print(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/\")\n","conf.get_default().auth_token = getpass.getpass()\n","\n","# Setup a tunnel to the streamlit port 8050\n","public_url = ngrok.connect(8050)\n","\n","print(public_url)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J_YcHca4HBHs"},"outputs":[],"source":["!pgrep bentoml\n","!kill $(pgrep bentoml)"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":0}
