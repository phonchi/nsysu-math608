{"cells":[{"cell_type":"markdown","metadata":{"id":"ZguKQsaEtw45"},"source":["# Assignment 1"]},{"cell_type":"markdown","metadata":{"id":"RxvUeukG506E"},"source":["#### Student ID: *Double click here to fill the Student ID*\n","\n","#### Name: *Double click here to fill the name*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gI6a-hc4VIc4"},"outputs":[],"source":["%pip install --pre pycaret[full] -qq\n","%pip install cleanlab -qq\n","%pip install snorkel -qq"]},{"cell_type":"markdown","metadata":{"id":"aBDkj0NJ18kk"},"source":["## Q1: Exploring a Low-Code ML Framework with Melbourne Housing dataset"]},{"cell_type":"markdown","metadata":{"id":"fCntJOu5qJkQ"},"source":["After completing the California census data project, we will shift our focus to a different housing dataset.\n","\n","The dataset is a snapshot of a [dataset](https://www.kaggle.com/datasets/anthonypino/melbourne-housing-market) created by Tony Pino. This dataset is a compilation of housing market data for Melbourne, which was sourced from weekly public listings on [Domain.com.au](https://www.domain.com.au/). It includes various details such as the address, type of real estate, suburb, selling method, number of rooms, price, real estate agent, date of sale, and the distance from the central business district, among others. **Ensure to set the random seed to 2024 for reproducibility.**\n","\n","To begin, execute the following code snippet for data preparation:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CuXdXPyheslS"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from pycaret.regression import *\n","\n","# Prepare data\n","df = pd.read_csv(\"melb_data.csv\")\n","df = df.drop(['Date', 'Address'], axis=1)\n","df = df.copy()\n","df"]},{"cell_type":"markdown","metadata":{"id":"k3Ff3rtBnklY"},"source":["**(a) Data Splitting**: Using the `train_test_split()` function, divide the dataset into two parts: a training set and a test set. Allocate 5% of the data to the test set, which will be used for testing purposes. (5%)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rsuScsCin-oE"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"XvzpnzpQqqlx"},"source":["**(b) Dataset Preparation Using PyCaret:**\n","Prepare the dataset for our machine learning model using the `setup()` function from `PyCaret` with the following requirements: (10%)\n","\n","- **Features**: Include all features in the `df` dataframe, except for the `Price` column, which will be used as the target.\n","- **Target**: Set `Price` as the target variable.\n","- **Validation Set**: Split 20% of the data into a validation set to evaluate the model.\n","- **Missing Values**:\n","  - For numerical variables, replace missing values with the median value of the respective columns. Assume that columns of data type `object` are categorical variables, and all others are numerical.\n","  - For categorical variables, replace missing values with the most frequent value in each column.\n","- **Encoding**:\n","  - Apply one-hot encoding to categorical variables that have fewer than 10 categories.\n","  - Use target encoding for categorical columns with 10 or more categories.\n","- **Standardization**: All features should be standardized. Note that PyCaret automatically scales all features by default.\n","\n","**Hint:** Refer to the PyCaret documentation [here](https://pycaret.readthedocs.io/en/latest/api/regression.html) for additional details on setup parameters. Be aware of how PyCaret handles feature scaling by default as discussed [here](https://github.com/pycaret/pycaret/issues/3076)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hEclHDblx_7y"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"8fI83nw-5j_i"},"source":["After setting up your dataset with `PyCaret`, you can review the configuration and steps of the created machine learning pipeline by executing the following code snippet. This will help you verify that all preprocessing steps are correctly applied as per the requirements:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oNz3shMXfZ9X"},"outputs":[],"source":["# Get the PyCaret configuration\n","config = get_config('pipeline')\n","config"]},{"cell_type":"markdown","metadata":{"id":"G-Llf3dx-p7e"},"source":["**(c) Verify Transformed Data:**\n","Use the `get_config()` function in `PyCaret` to inspect the transformed dataset. After executing this function, review the output to ensure the number of samples and the number of features match your expectations based on the data preparation steps previously outlined. (10%)\n","\n","Provide a brief explanation of why the numbers match or differ from your expectations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O1H0FVaIbX-q"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"U2NamG2QD05O"},"source":["> Ans: *double click here to answer the question.*"]},{"cell_type":"markdown","metadata":{"id":"8fSsEjKA88Jc"},"source":["**(d) Model Comparison and Evaluation:** (10%)\n","\n","Utilize the `compare_models()` function in `PyCaret` to conduct a 3-fold cross-validation analysis. Compare the performance of the following machine learning models:\n","\n","- Linear Regression\n","- Lasso Regression\n","- Ridge Regression\n","- Support Vector Regression (SVR)\n","- K-Nearest Neighbor (KNN) Regressor\n","- Random Forest Regressor\n","- XGBoost\n","- LightGBM\n","- CatBoost\n","\n","List the top five models according to their performance on the Mean Absolute Percentage Error (MAPE) measure."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"teo2hgUzBqnH"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"cio2CzZFEB8u"},"source":["> Ans: *double click here to answer the question.*"]},{"cell_type":"markdown","metadata":{"id":"X1TICQ99_TbP"},"source":["**(e) Final Model Selection and Evaluation**: Using the `create_model()` function in `PyCaret`, select the best-performing model from part (d) based on the MAPE metric. Evaluate this model on the test set that was reserved in part (a). Report the MAPE on the test set to assess the modelâ€™s performance. (5%)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1VOS1NxEnCvA"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"sKTSX0pmEC-O"},"source":["> Ans: *double click here to answer the question.*"]},{"cell_type":"markdown","metadata":{"id":"XHmRCbF1VtRO"},"source":["## Q2: Handling an Unlabeled Dataset in a Startup Environment"]},{"cell_type":"markdown","metadata":{"id":"Tzp0m9WpYTUb"},"source":["In this exercise, you'll hone your data preparation skills, a crucial element in many real-world projects. You are provided with an unlabeled dataset `data.csv`, which consists of two features and encompasses 300 sample points. Each sample point is associated with one of four distinct classes. Additionally, you have `crowdsourcing.csv` containing labels from 50 different workers for these sample points.\n","\n","**While this is a practice project, and you have access to the actual ground truth labels in `labels.csv`, these labels are restricted to measuring the accuracy of your models. Under no circumstances should the ground truth labels be used in other parts of the training process.** Ensure to set the random seed to 2024 for all operations requiring randomness to maintain consistency in your results."]},{"cell_type":"markdown","metadata":{"id":"dRV_HimuEXHl"},"source":["Firstly, read the dataset and divide it into training and testing sets using the following code snippet:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_C1GOcN9ze_t"},"outputs":[],"source":["X = pd.read_csv('data.csv', names=[0,1])\n","true_labels = np.loadtxt(\"labels.csv\", delimiter=',')\n","multiannotator_labels = pd.read_csv('crowdsourcing.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UdnFtkrvdHgb"},"outputs":[],"source":["X_train, X_test, multiannotator_labels, ano_unseen, Y_train, Y_test  = train_test_split(X, multiannotator_labels, true_labels, test_size=0.1, random_state=2024)"]},{"cell_type":"markdown","metadata":{"id":"RSa3-5NDIKmB"},"source":["**(a) Label Aggregation Using Snorkel:** (10%)\n","\n","In this task, we'll explore the use of `Snorkel` to manage and refine crowdsourced labels. Each crowd worker will be treated as an individual labeling function since each one labels different subsets of the dataset and might introduce errors or conflicts.\n","\n","**Steps:**\n","\n","1. **Convert Crowdsourcing Data**: Transform the crowdsourcing data frame into a label matrix where each row corresponds to a sample and each column to a worker's label for that sample. This matrix will be used to train the `Snorkel` model.\n","\n","2. **Train the Label Model**: Use the `Snorkel` `LabelModel` to train on the label matrix you created. Apply the following hyperparameters:\n","   ```python\n","   fit(L_train, n_epochs=500, seed=2024, log_freq=20, l2=0.1, lr=0.001, lr_scheduler=\"linear\", optimizer=\"adam\")\n","   ```\n","\n","3. **Generate Predictions and Evaluate Accuracy**: Once trained, use the `LabelModel` to generate predictions for each training sample. Then, calculate the accuracy of these predictions by comparing them against the ground truth labels from `labels.csv`. Remember, the ground truth is only used for evaluating accuracy, not for training the Label Model.\n","\n","**Hints:**\n","- You can check the Snorkel documentation [here](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.model.label_model.LabelModel.html) for more details on the `LabelModel` parameters and methods."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H3h1IOR5IU53"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"nMc3k_PPJlgA"},"source":["**(b) Refining Labels with Cleanlab:** (10%)\n","\n","In this second trial, we'll leverage the `cleanlab` library to further analyze and refine the labels obtained from crowdsourcing. The process will involve deriving consensus labels from the crowdsourced data, using majority voting and model predictions.\n","\n","**Steps:**\n","\n","1. **Majority Vote Labels**:\n","   - Utilize the `get_majority_vote_label()` function to aggregate the labels from different workers for each sample into a single majority vote label.\n","   \n","2. **Model Predictions**:\n","   - Apply a 5-fold cross-validation on the training dataset labeled with the majority vote results. Use the model below to compute out-of-sample predicted probabilities for each sample.\n","   ```python\n","    LogisticRegression(solver='liblinear', penalty='l1', C=0.1, random_state=2024)\n","   ```\n","\n","3. **Consensus Labels**:\n","   - Employ the `get_label_quality_multiannotator()` function from `cleanlab` to combine the out-of-sample predicted probabilities with the crowdsourced labels. This function will help generate consensus labels, which aim to be more accurate by considering both worker disagreement and model confidence.\n","\n","4. **Evaluate Accuracy**:\n","   - Calculate the accuracy of both the majority vote labels and the consensus labels by comparing them against the ground truth labels in `labels.csv`. As with previous tasks, remember that the ground truth is strictly for evaluation purposes.\n","\n","**Hints:**\n","- Explore the Cleanlab documentation [here](https://github.com/cleanlab/cleanlab) for additional insights into functions like `get_majority_vote_label()` and `get_label_quality_multiannotator()`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9-HvBXBhHmm9"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"R9Y2DByyKINF"},"source":["**(c) Evaluate Random Forest Classifiers:** (10%)\n","\n","Since we don't have these crowdsourcing labels for the test set or new incoming data points, we can't use the `LabelModel` or obtain consensus label at inference time. In order to run inference on new incoming data points, we need to train a discriminative model! Train a model using the following setup:\n","\n","```python\n","RandomForestClassifier(random_state=2024, n_estimators=10)\n","```\n","**Steps:**\n","- Train a classifier for each type of label.\n","    - **Snorkel Labels**: Generated in part (a).\n","    - **Majority-Vote Labels**: Obtained in part (b) from `get_majority_vote_label()`.\n","    - **Consensus Labels**: Derived in part (b) from `get_label_quality_multiannotator()`.\n","- Evaluate and report the classification accuracy on the test set.\n","- Determine which classifier performs best based on the accuracy scores."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IF9TqRlRiEE5"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"_O8wzGV-Epe-"},"source":["> Ans: *double click here to answer the question.*"]},{"cell_type":"markdown","metadata":{"id":"le9kFq4PMsLf"},"source":["**(d) Enhanced Random Forest Training:** (10%)\n","\n","Train two additional Random Forest classifiers with the specifications:\n","```python\n","RandomForestClassifier(random_state=2024, n_estimators=10)\n","```\n","\n","**Steps:**\n","- Fit each classifier using the respective sample weights for the labels.\n","    - **Snorkel Labels**: Use `inv_entropy()` to calculate weights based on prediction probabilities from the `Snorkel` `LabelModel`.\n","    - **Consensus Labels**: Use `consensus_quality_score` from `Cleanlab` for weights.\n","- Evaluate and report the classification accuracy on the test set.\n","- Compare the results to those from (c) to assess performance improvement.\n","\n","Hint: Refer to [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit) for the `sample_weight` paprameter."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2bN2t1pMvZ4"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"KcyC4CAFEtom"},"source":["> Ans: *double click here to answer the question.*"]},{"cell_type":"markdown","metadata":{"id":"Qhbc-fs3mc4g"},"source":["## Q3: Analyze StackOverflow dataset using SQL"]},{"cell_type":"markdown","metadata":{"id":"FZWBn2fiCh0J"},"source":["Kaggle has a rich number of [BigQuery](https://www.kaggle.com/datasets?fileType=bigQuery) and [SQLite](https://www.kaggle.com/datasets?fileType=sqlite) datasets that you can practice your SQL skill.\n","\n","In this question, we are going to examine the StackOverflow dataset. [Stack Overflow](https://stackoverflow.com/) is a popular question and answer site for technical questions.\n","\n","Hint: It is recommended to answer this question in Kaggle, where you can access the dataset directly."]},{"cell_type":"markdown","metadata":{"id":"w9iXYvI7gHDx"},"source":["Firstly, if you are using colab, use the following code snippet to setup the client. For more detail, please refer to our lab."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"_ArDmIuhJpdM"},"outputs":[],"source":["from google.cloud import bigquery\n","from google.oauth2 import service_account\n","import sys\n","\n","# Upload JSON file here\n","from google.colab import files\n","uploaded = files.upload()\n","\n","path_of_json = 'nomadic-botany-435608-b6-e1a2d53f264a.json'  #@param {type: \"string\"}\n","\n","# TODO(developer): Set path_of_json to the path to the service account key file.\n","\n","credentials = service_account.Credentials.from_service_account_file(\n","    path_of_json\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a_f_cHz6nH4I"},"outputs":[],"source":["if \"google.colab\" in sys.modules:\n","    client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n","else:\n","    # Below and create a \"Client\" object if you are using Kaggle\n","    client = bigquery.Client()"]},{"cell_type":"markdown","metadata":{"id":"9batOLKbJmLR"},"source":["**(a) Initial Exploration of the Stack Overflow Dataset:** (10%)\n","\n","Begin your analysis by familiarizing yourself with the structure and contents of the Stack Overflow dataset:\n","\n","1. **List All Tables**: Print the names of all tables available in the dataset.\n","2. **View Table Schemas**: Display the schema for the `posts_questions` and `posts_answers` tables.\n","3. **Preview Data**: Retrieve and preview the first ten rows from each of the tables mentioned above.\n","\n","**Hint:** Use the following code snippet to reference the dataset in your client setup:\n","```python\n","dataset_ref = client.dataset(\"stackoverflow\", project=\"bigquery-public-data\")\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lB3YsScAw28g"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"qLJkoaPELjzq"},"source":["**(b) Daily Post Counts for 2020:** (10%)\n","\n","Write an SQL query to count the number of posts created each day in the `posts_questions` table for the year 2020:\n","\n","- **Filter by Year**: Use `WHERE` and `EXTRACT()` to select posts from 2020 only.\n","- **Date Column**: Format the `creation_date` in DATE format (e.g. 2021-01-01) and rename it to \"created_day\". (Remember the SELECT... from clause, DATE() function and `AS` command).\n","- **Count Posts**: Count the daily posts and label this column as \"number_of_posts\". (Remember the `COUNT()` function).\n","- **Group and Sort**: Group the results by \"created_day\" using `GROUP BY` and order them by \"created_day\" in ascending order using `ORDER BY`.\n","\n","Your query should return a dataframe with 366 rows and two columns, showing the date and corresponding post count.\n","\n","<center>\n","\n","| created_day | number_of_posts |\n","|:-----------:|:---------------:|\n","|  2020-01-01 |       2390      |\n","|  2020-01-02 |       4601      |\n","|  2020-01-03 |       4816      |\n","|     ...     |       ....   |\n","|  2020-12-31 |       3498      |\n","\n","</center>\n","\n","Generate a line plot with \"created_day\" on the x-axis and \"number_of_posts\" on the y-axis to visualize daily posting trends. What patterns do you observe from the line plot regarding post frequency throughout 2020?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Id0aY2dzr0OS"},"outputs":[],"source":["# coding your answer here.\n","query = \"\"\"\n","     \"\"\"\n","\n","############   Do not modify the code below     ############\n","# Set up the query (cancel the query if it would use too much of\n","# your quota, with the limit set to 1 GB)\n","safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n","query_job = client.query(query, job_config=safe_config)\n","\n","# API request - run the query, and return a pandas DataFrame\n","results = query_job.to_dataframe()\n","############   Do not modify the code above     ############"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mF7oFgDutCaJ"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"WkALZDiz2rkB"},"source":["> Ans: *double click here to answer the question.*"]},{"cell_type":"markdown","metadata":{"id":"3d5qD12pL8nB"},"source":["**(c) Analyzing User Engagement with 'pandas' Tag**: (10%)\n","\n","`posts_questions` has a column called tags which lists the topics/technologies each question is about. `posts_answers` has a column called `parent_id` which identifies the ID of the question each answer corresponds to. You can then join two tables by the `parent_id` in `posts_answers` and the `id` in `posts_questions`. `posts_answers` also has an `owner_user_id` column which specifies the ID of the user who answered the question.\n","\n","Now, write a query that has a single row for each user who answered at least one question with a tag that equals to string `pandas`. Your results should have two columns (Remember the `SELET...WHERE` clause) and follow the below restrictions:\n","\n","* `owner_user_id` - contains the `owner_user_id` column from the `posts_answers` table (Remember the `INNER JOIN` clause)\n","* `number_of_answers` - contains the number of answers the user has written to `pandas` related questions using the `tage` column (Remember the `AS`, `GROUP BY` and `COUNT` clause)\n","* Discard the rows whose `number_of_answers` is smaller or equal to 3 (Remember the `HAVING` clause)\n","\n","<center>\n","\n","| owner_user_id | number_of_answers |\n","|---------------|-------------------|\n","| 1234567       | 794               |\n","| 2345678       | 304               |\n","|     ...     |       ....   |\n","|  9999999 |       4      |\n","\n","</center>\n","\n","In the retrieve data frame, which user answers most questions in the pandas domain? Try to find the email of the user.\n","\n","Hint: You can find the public user profile by appending the ID after https://stackoverflow.com/users/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NipS2aIIMC0i"},"outputs":[],"source":["# coding your answer here.\n","query = \"\"\"\n","        \"\"\"\n","\n","############   Do not modify the code below     ############\n","# Set up the query (cancel the query if it would use too much of\n","# your quota, with the limit set to 1 GB)\n","safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n","query_job = client.query(query, job_config=safe_config)\n","\n","# API request - run the query, and return a pandas DataFrame\n","results = query_job.to_dataframe()\n","############   Do not modify the code above     ############"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7xQDjw-Qy0_A"},"outputs":[],"source":["# coding your answer here."]},{"cell_type":"markdown","metadata":{"id":"N1PMRwwY2mNq"},"source":["> Ans: *double click here to answer the question.*"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":0}